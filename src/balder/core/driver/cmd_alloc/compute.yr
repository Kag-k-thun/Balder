in compute;

use balder::core::{memory::_, driver::_};
use balder::core::{error, dispose};

use ::vulkan::core;
use std::io;

enum
| LOADING = 0u32
| DISPATCH = 1u32
 -> CommandAllocatorPhase;

/**
 * The indirect command allocator is used to call compute shaders
 */
@final
pub class ComputeDispatchCommandAllocator over CommandAllocator {

    // The phase of the command allocator
    let mut _phase : CommandAllocatorPhase = CommandAllocatorPhase::LOADING;
    
    // The list of dispatch indirect commands
    let mut _dispatchCmds : [mut VkDispatchIndirectCommand] = [];

    // The id of the current command being dispatched
    let mut _current : u32 = 0;

    // The buffer storing information about the indirect dispatch
    let dmut _dispatchCmdBuffer : (&Buffer)? = none;

    let mut _width : u32 = 1;
    let mut _height : u32 = 1;
    let mut _depth : u32 = 1;
    
    
    /**
     * Crete an empty indirect command allocator
     * @params:
     *    - device: the device used to allocate and draw
     */
    pub self (dmut pass : &ComputeSubpass)
        with super (alias pass)
        , _width = pass.getDimension ()._0
        , _height = pass.getDimension ()._1
        , _depth = pass.getDimension ()._2    
    {}                   

    /**
     * ======================================================================================================
     * ======================================================================================================
     * ==========================================   LOADING PHASE   =========================================
     * ======================================================================================================
     * ======================================================================================================
     */

    /**
     * Register an indexed mesh that will be drawn during the draw phase
     * @params:
     *    - groupX : the size of the local groups in the compute shader being called
     *    - groupY : the size of the local groups in the compute shader being called
     *    - groupZ : the size of the local groups in the compute shader being called
     */
    pub fn registerDispatch (mut self, groupX : u32 = 1, groupY : u32 = 1, groupZ : u32 = 1) {
        let mut cmd = VkDispatchIndirectCommand ();
        cmd.x = self._width / groupX  + if (self._width % groupX != 0) { 1u32 } else { 0u32 };
        cmd.y = self._height / groupY + if (self._width % groupX != 0) { 1u32 } else { 0u32 };
        cmd.z = self._depth / groupZ  + if (self._width % groupX != 0) { 1u32 } else { 0u32 };        
        
        self._dispatchCmds ~= [cmd];
    }

    /**
     * Finalize the loading phase of the command buffer allocator. So it enter the drawing phase.
     */
    pub fn finalizeRegister (mut self)
        throws BalderError
    {
        self._phase = CommandAllocatorPhase::DISPATCH;
        if (self._dispatchCmds.len != 0us) {
            let flag = cast!u32 (VkBufferUsageFlagBits::VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT |
                                 VkBufferUsageFlagBits::VK_BUFFER_USAGE_TRANSFER_DST_BIT);

            let dmut buf = self._device:.getMemoryAllocator ():.allocBuffer (self._dispatchCmds.len * VkDispatchIndirectCommand::size,
                                                                             flag,
                                                                             cast!u32 (VkMemoryPropertyFlagBits::VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT));

            buf:.update (self._dispatchCmds);
            self._dispatchCmdBuffer = alias (alias buf)?;
            self._dispatchCmds = [];
            self._current = 0;
        }
    }

    /**
     * ======================================================================================================
     * ======================================================================================================
     * ==========================================   DRAWING PHASE   =========================================
     * ======================================================================================================
     * ======================================================================================================
     */

    /**
     * Draw the next indexed buffer
     * @assume: associated buffers, shaders and descriptor set are correctly bound
     */
    pub fn dispatchNext (mut self) {
        if let Ok (dmut buf) = alias self._dispatchCmdBuffer {
            vkCmdDispatchIndirect (commandBuffer-> self.getCommandBuffer (),
                                   buffer-> buf.getVulkanBuffer (),
                                   offset-> cast!u64 (self._current * VkDispatchIndirectCommand::size));
            
            self._current += 1;
        }
    }

    /**
     * ======================================================================================================
     * ======================================================================================================
     * ============================================   DISPOSING   ===========================================
     * ======================================================================================================
     * ======================================================================================================
     */

    impl Disposable {
        pub over dispose (mut self) {
            if let Ok (dmut buf) = alias self._dispatchCmdBuffer {
                buf:.dispose ();
                self._dispatchCmdBuffer = none;
            }

            self._dispatchCmds = [];
            self._current = 0;
            self._phase = CommandAllocatorPhase::LOADING;
            
            self.__super__:.dispose ();
        }
                
    }
    
}
